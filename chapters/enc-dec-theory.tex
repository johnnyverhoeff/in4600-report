%!TEX root = ../thesis.tex

\section{Encoding \& Decoding Data Simulation}
\label{sec:enc-dec-theory}

	All of the coding schemes as discussed in \autoref{sec:CDMA} are designed for wireless transceivers.
	Meaning they send a radio wave through the air which is encoded with that particular code sequence.
	That radio wave is an analog signal and can vary between $+1$ and $-1$ symbols.
	But the VLC enabled LEDs cannot send such signals.
	They can only be turned on or off, thus encoding only $0$ and $1$ symbols.
	Those symbols correspond to a current draw of the LED which is measured by an ADC for example.
	To experiment with the encoding and decoding process and obstacles without the need for hardware, we use Matlab to simulate the process.

	\subsection{Walsh-Hadamard Sequences}

		%about radio dec. and bin. enc. proof etc...
		The way the Walsh-Hadamard sequences work with encoding and decoding with for example the link from a base-station to a mobile device of a CDMA application, is that the sequence of $+1$ and $-1$ symbols gets multiplied with the encoded data. 
		Say for example that we have a Hadamard matrix of rank $8$ as can be seen in \autoref{matrix:h8} which is created by using \autoref{eq:hadamard-matrix-creation}.
		And we select a random code to encode our data with, which corresponds to a row of this matrix.
		We also assume that our data is binary, consisting of 0s and 1s. 
		Our last assumption is that is we want to encode a $0$ we use the code itself and if we want to encode a $1$ we use the inverse of the code, i.e. $-1 \times$ the code.
		That describes the encoding process. 
		The decoding process involves the calculation of the correlation of the received signal with a code sequence for which we want to know if there is information embedded in the received signal and if so, what might that information be.

			\begin{equation}
				H_8 = 
				\begin{bmatrix*}[r] 
					H_4 & H_4 \\ 
					H_4 & -H_4 
				\end{bmatrix*}
				%=
				%\begin{bmatrix*}[r] 
				%	H_2 & H_2   & H_2 & H_2   \\
				%	H_2 & -H_2  & H_2 & -H_2  \\
				%	H_2 & H_2   & -H_2 & -H_2 \\
				%	H_2 & -H_2  & -H_2 & H_2  \\
				%\end{bmatrix*}
				=
				\dotsc
				=
				\begin{bmatrix*}[r]
					1	&	 1	&	 1	&	 1	&	 1	&	 1	&	 1	&	 1 \\
					1	&	-1	&	 1	&	-1	&	 1	&	-1	&	 1	&	-1 \\
					1	&	 1	&	-1	&	-1	&	 1	&	 1	&	-1	&	-1 \\
					1	&	-1	&	-1	&	 1	&	 1	&	-1	&	-1	&	 1 \\
					1	&	 1	&	 1	&	 1	&	-1	&	-1	&	-1	&	-1 \\
					1	&	-1	&	 1	&	-1	&	-1	&	 1	&	-1	&	 1 \\
					1	&	 1	&	-1	&	-1	&	-1	&	-1	&	 1	&	 1 \\
					1	&	-1	&	-1	&	 1	&	-1	&	 1	&	 1	&	-1 
				\end{bmatrix*}
				\label{matrix:h8}
			\end{equation}

		Since the autocorrelation of Walsh-Hadamard sequences do not allow to find the beginning of a code, see \autoref{fig:autocorr-hadamard}, we will only consider the synchronized situation, for which these codes are also used in practice, so $\tau = 0$.
		Say we have a code $c_i(t)$ and we want to encode a $0$ with this code and send that as a signal for our receiver to receive.
		The receiver will receive this signal $s(t)$ assuming a perfect channel, the decoding process can be seen below.

		%Want something different than proof, but dont know how/what

		\begin{proof}
			Let $s(t)$ be the received signal which encodes a 0 for code: $c_i(t)$.\\
			And let $c_i(t)$ be the code for which we want to check if there is information there. \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c_i(t)	
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c_i(t)	
				\\ s(t) = c_i(t)															
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c_i(t) \times c_i(t) = L
			\end{align*}

		\end{proof}

		We can also state what the result will be when we decode a $1$ with a code $c_i(t)$, see below:

		\begin{proof}
			Let $s(t)$ be the received signal which encodes a 1 for code: $c_i(t)$.\\
			And let $c_i(t)$ be the code for which we want to check if there is information there. \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c_i(t)	
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c_i(t)	
				\\ s(t) = -1 \times c_i(t)															
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} -1 \times c_i(t) \times c_i(t)
				\\ &= -1 \times \displaystyle\sum_{t = 0} ^ {L - 1} c_i(t) \times c_i(t) = -1 \times L
			\end{align*}

		\end{proof}

		This two statement state that if we encode a data bit with code $c_i$ and that is the only information received by the receiver, we either get the maximum positive correlation, i.e. $L$ or we get the maximum negative correlation, i.e. $-L$.
		If we would have received a signal consisting of only zeros the correlation would also be zero.

		Next let us look at what happens when we try to decode information with a different code than the code which was used to encode information with. In that case we also get a correlation of zero, see below. This is per definition the case since we are using Walsh-Hadamard codes which are orthogonal to each other when they are synchronized.

		\begin{proof}
			Let $s(t)$ be the received signal which encodes a data bit for code: $c_i(t)$.\\
			And let $c_j(t)$ be the code for which we want to check if there is information there. \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c_j(t)	
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c_j(t)	
				\\ s(t) = c_i(t)															
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c_i(t) \times c_j(t) = 0
			\end{align*}

		\end{proof}

		Finally we look at the decoding process for when the received signal consists of several encoded signals, see below. 
		Because all the codes in Walsh-Hadamard sequences are orthogonal to each other they produce no MAI and so there can be as many codes superimposed on each other and still a correct decoding can take place. 


		\begin{proof}
			Let $s(t)$ be the received signal which encodes data for several codes: $c_i(t)$, $c_j(t)$, $c_k(t)$ and $c_l(t)$.\\
			And let $c_i(t)$ be the code for which we want to check if there is information there. \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c_i(t)	
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c_i(t)	
				\\ s(t) = c_i(t) + c_j(t) + c_k(t) + c_l(t)															
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} \{ c_i(t) + c_j(t) + c_k(t) + c_l(t) \} \times c_i(t)
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c_i(t) \times c_i(t) + c_j(t) \times c_i(t) + c_k(t) \times c_i(t) + c_l(t) \times c_i(t)
				\\ = L + 0 + 0 + 0 = L
			\end{align*}

		\end{proof}

		But as stated in \autoref{sec:enc-dec-theory} the VLC enabled LEDs cannot send $+1$ and $-1$ symbols of data, they can only be in an on or off state.
		This means that the code that is being used for encoding must be mapped to zeros and ones.
		Let us assume that a $+1$ symbol maps to an off state, i.e. $0$ and that a $-1$ symbol maps to an on state, i.e. $1$.
		We can summarize that in \autoref{eq:radio-to-bin}, where $r$ denotes the $+1$ or $-1$ symbols and the outcome $b$ will be our binary value, i.e. $0$ or $1$.

		\begin{equation}
			b = \frac{1 - r}{2}
			\label{eq:radio-to-bin}
		\end{equation}

		Because we are now using the binary values as code, we can no longer use multiply to encode our binary information.
		Instead, we must use the XOR function.
		Now let us see what happens when we try to decode information that has been encoded using on-off keying (OOK), below is stated what happens when both data bits are encoded.

		\begin{proof}
			Let $s(t)$ be the received signal which encodes a 0 for code: $c^b_i(t)$, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\
			And let $c^r_i(t)$ be the code for which we want to check if there is information there, where $c^r_i(t)$ uses the traditional values $-1$ and $+1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^r_i(t)	
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^r_i(t)	
				\\ s(t) = \forall t \in \{ 0, 1, 2, \dotsc, L - 1 \} \ \ 0(t) \oplus c^b_i(t) = c^b_i(t) \tag{Where $0(t) = 0 \ \forall t$ }										
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c^b_i(t) \times c^r_i(t)
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \frac{1 - c^r_i(t)}{2} \times c^r_i(t) \tag{Via \autoref{eq:radio-to-bin}}
				\\ = \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) - c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) - \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t)
				\\ = 0 - \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t) \tag{Balance property}
				\\ = - \frac{1}{2} \times L
			\end{align*}

		\end{proof}

		\begin{proof}
			Let $s(t)$ be the received signal which encodes a 1 for code: $c^b_i(t)$, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\
			And let $c^r_i(t)$ be the code for which we want to check if there is information there, where $c^r_i(t)$ uses the traditional values $-1$ and $+1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^r_i(t)	
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^r_i(t)	
				\\ s(t) = \forall t \in \{ 0, 1, 2, \dotsc, L - 1 \} \ \ 1(t) \oplus c^b_i(t) = 1 - c^b_i(t) \tag{Where $1(t) = 1 \ \forall t$ }
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} (1 - c^b_i(t)) \times c^r_i(t)
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} (1 - \frac{1 - c^r_i(t)}{2}) \times c^r_i(t) \tag{Via \autoref{eq:radio-to-bin}}
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \frac{1 + c^r_i(t)}{2} \times c^r_i(t)
				\\ = \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) + c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) + \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t)
				\\ = 0 + \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t) \tag{Balance property}
				\\ = \frac{1}{2} \times L
			\end{align*}

		\end{proof}

		The case for when there is no data encoded for this particular code:

		\begin{proof}
			Let $s(t)$ be the received signal which encodes data for code: $c^b_i(t)$, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\
			And let $c^r_j(t)$ be the code for which we want to check if there is information there, where $c^r_j(t)$ uses the traditional values $-1$ and $+1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^r_j(t)	
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^r_j(t)	
				\\ s(t) = c^b_i(t)										
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c^b_i(t) \times c^r_j(t)
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \frac{1 - c^r_i(t)}{2} \times c^r_j(t) \tag{Via \autoref{eq:radio-to-bin}}
				\\ = \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_j(t) - c^r_i(t) \times c^r_j(t)
				\\ = \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_j(t) - \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_j(t)
				\\ = 0 - \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t) \tag{Balance property}
				\\ = 0 + 0 = 0\tag{Because the codes are orthogonal.}
			\end{align*}

		\end{proof}

		Finally the case for when there are multiple signals:


		\begin{proof}
			Let $s(t)$ be the received signal which encodes data for several codes: $c^b_i(t)$, $c^b_j(t)$, $c^b_k(t)$ and $c^b_l(t)$, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$\\
			And let $c^r_i(t)$ be the code for which we want to check if there is information there, where $c^r_i(t)$ uses the traditional values $-1$ and $+1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^r_i(t)	
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^r_i(t)	
				\\ s(t) = c^b_i(t) + c^b_j(t) + c^b_k(t) + c^b_l(t)															
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} \{ c^b_i(t) + c^b_j(t) + c^b_k(t) + c^b_l(t) \} \times c^r_i(t)
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c^b_i(t) \times c^r_i(t) + c^b_j(t) \times c^r_i(t) + c^b_k(t) \times c^r_i(t) + c^b_l(t) \times c^r_i(t)
				\\ = \frac{1}{2} \times L + 0 + 0 + 0 = \frac{1}{2} \times L \tag{See above proofs}
			\end{align*}

		\end{proof}




		As we can see the correlation is for both data bits and when there is no information encoded, exactly $-\frac{1}{2}$ times as large as when dealing only with $+1$ and $-1$ symboled codes. 
		It also works for when there are multiple streams of encoded information present in the signal received.
		A summary is available, see \autoref{tbl:correlation-levels}.

		\begin{table}[h!]
			\centering
			
			\begin{tabular}{| l | l |}
				\hline
				Method												& Correlation \\ \hline \hline
				Encoding a 0 and decoding with only one transmitter & $-\frac{1}{2} \times L$  \\ \hline
				Encoding a 1 and decoding with only one transmitter & $ \frac{1}{2} \times L$   \\ \hline
				Encoding a 0 and decoding with a different code, with only one transmitter & 0  \\ \hline
				Encoding a 0 and decoding with multiple transmitters & $-\frac{1}{2} \times L$ \\ \hline
			\end{tabular}
			\caption{Table containing summary of correlation levels.}
			\label{tbl:correlation-levels}
		\end{table}









		We can also choose to encode and decode with the binary valued codes only, so only use values with $0$ and $1$. 


		\begin{proof}
			Let $s(t)$ be the received signal which encodes a 0 for code: $c^b_i(t)$, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\
			And let $c^b_i(t)$ be the code for which we want to check if there is information there, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^b_i(t)	
				\\ R(0)_{sc^b_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^b_i(t)	
				\\ s(t) = \forall t \in \{ 0, 1, 2, \dotsc, L - 1 \} \ \ 0(t) \oplus c^b_i(t) = c^b_i(t) \tag{Where $0(t) = 0 \ \forall t$ }
				\\ R(0)_{sc^b_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c^b_i(t) \times c^b_i(t)
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \frac{1 - c^r_i(t)}{2} \times \frac{1 - c^r_i(t)}{2} \tag{Via \autoref{eq:radio-to-bin}}
				\\ = \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} 1 - 2 \times c^r_i(t) + c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} 1 - \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) + \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{4} \times L - 0 + \frac{1}{4} \times L = \frac{1}{2} \times L
			\end{align*}

		\end{proof}


		\begin{proof}
			Let $s(t)$ be the received signal which encodes a 1 for code: $c^b_i(t)$, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\
			And let $c^b_i(t)$ be the code for which we want to check if there is information there, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^b_i(t)	
				\\ R(0)_{sc^b_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^b_i(t)	
				\\ s(t) = \forall t \in \{ 0, 1, 2, \dotsc, L - 1 \} \ \ 1(t) \oplus c^b_i(t) = 1 - c^b_i(t) \tag{Where $1(t) = 1 \ \forall t$ }
				\\ R(0)_{sc^b_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} (1 - c^b_i(t)) \times c^b_i(t)
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} (1 - \frac{1 - c^r_i(t)}{2}) \times \frac{1 - c^r_i(t)}{2} \tag{Via \autoref{eq:radio-to-bin}}
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \frac{1 + c^r_i(t)}{2} \times \frac{1 - c^r_i(t)}{2}
				\\ = \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} 1 - c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} 1 - \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{4} \times L - \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{4} \times L - \frac{1}{4} \times L = 0
			\end{align*}

		\end{proof}

		\begin{proof}
			Let $s(t)$ be the received signal which encodes a 0 for code: $c^b_i(t)$, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\
			And let $c^b_j(t)$ be the code for which we want to check if there is information there, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^b_j(t)	
				\\ R(0)_{sc^b_{j}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^b_j(t)	
				\\ s(t) = \forall t \in \{ 0, 1, 2, \dotsc, L - 1 \} \ \ 0(t) \oplus c^b_i(t) = c^b_i(t) \tag{Where $0(t) = 0 \ \forall t$ }
				\\ R(0)_{sc^b_{j}} = \displaystyle\sum_{t = 0} ^ {L - 1} c^b_i(t) \times c^b_j(t)
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \frac{1 - c^r_i(t)}{2} \times \frac{1 - c^r_j(t)}{2} \tag{Via \autoref{eq:radio-to-bin}}
				\\ = \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} 1 - c^r_i(t) - c^r_j(t) + c^r_i(t) \times c^r_j(t)
				\\ = \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} 1 - \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) - \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_j(t) + \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_j(t) 
				\\ = \frac{1}{4} \times L - 0 - 0 + 0 = \frac{1}{4} \times L
			\end{align*}

		\end{proof}

		\begin{proof}
			Let $s(t)$ be the received signal which encodes data for codes: $c^b_j(t)$, $c^b_l(t)$, where $c^b_j(t)$ is binary valued, i.e. $0$ or $1$\\
			And let $c^b_i(t)$ be the code for which we want to check if there is information there, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^b_i(t)	
				\\ R(0)_{sc^b_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^r_i(t)	
				\\ s(t) = c^b_j(t) + c^b_l(t)														
				\\ R(0)_{sc^b_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} \{ c^b_j(t) + c^b_l(t) \} \times c^b_i(t)
				\\ R(0)_{sc^b_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c^b_j(t) \times c^b_i(t) + c^b_l(t) \times c^b_i(t)
				\\ = \frac{1}{4} \times L + \frac{1}{4} \times L = \frac{1}{2} \times L \tag{See above proofs}
			\end{align*}

		\end{proof}



		\begin{table}[h!]
			\centering
			
			\begin{tabular}{| l | l |}
				\hline
				Method												& Correlation \\ \hline \hline
				Encoding a 0 and decoding with only one transmitter 							& $\frac{1}{2} \times L$  \\ \hline
				Encoding a 1 and decoding with only one transmitter 							& $ 0 $   \\ \hline
				Encoding a 0 and decoding with a different code, with only one transmitter 		& $\frac{1}{4} \times L$  \\ \hline
				Encoding a 0 and decoding with a different code, with two transmitters			& $\frac{1}{2} \times L$  \\ \hline
			\end{tabular}
			\caption{Table containing summary of correlation levels.}
			\label{tbl:correlation-levels-bin}
		\end{table}

		As we can see from the summary in \autoref{tbl:correlation-levels-bin}, the correlation levels for decoding one transmitter with the correct code, is the same as the correlation level from decoding with a code that was not encoded.
		This forms a problem as we cannot distinguish correctly these cases.
		And as a result we cannot use this decoding scheme. 


		In the synchronous case, for which these codes are also used in practice, we can correctly encode and subsequently decode information. 
		Without experiencing false-positives and without false-negatives.
		There can also be as much transmitters transmitting concurrently without MAI as there are codes available, because the codes are orthogonal.
		The only downside to this scheme is that all of the transmitter have to work together, they have to transmit synchronously.





	\subsection{PN Sequences}

		As we know from \autoref{sec:theory-pn}, there is no particular bound on the cross-correlation between two or more PN sequences.
		This means that they will have MAI and that there is a limit on how many transmitters can transmit at the same time, with the the receiver still able to correctly decode the information without experiencing false-positives and/or false-negatives.

		Since there is no particular bound on the cross-correlation with PN sequences from the same length, they need to be calculated.
		The calculated results can be seen in \autoref{tbl:correlation-pn-families}.


		\begin{table}
			\centering
			\begin{tabular}{ | l | l | l | l | l | }

				\hline
				LFSR size 	& Code length	& Number of codes 	& Maximum cross-correlation & $m$	\\ \hline

				3			& 7				& 2					& 5							& 0.70	\\ \hline	
				4			& 15			& 2					& 7							& 1.07	\\ \hline
				5			& 31			& 6					& 11						& 1.41	\\ \hline
				6			& 63			& 6					& 23						& 1.37	\\ \hline
				7			& 127			& 18				& 43						& 1.48	\\ \hline
				8			& 255			& 16				& 95						& 1.34	\\ \hline	


			\end{tabular}
			\caption{Table containing the absolute maximum cross-correlation per PN code family.}
			\label{tbl:correlation-pn-families}
		\end{table}

		If we want to calculate how many simultaneous transmitters can transmit, we need to set a threshold $T$ for which we will accept or reject a correlation as being a valid transmitter or not.
		We also know the correlation level of when a code is present in the signal, this is equal to $L$, where $L$ is the length of the code.
		The last thing we need is the absolute maximum cross-correlation for a given PN family of sequences and we can calculate the maximum number of concurrent transmitters.
		To prevent false negatives, i.e. there is a valid code present but it is lost due to for example MAI, the correlation level needs to be higher than the threshold $T$. 
		And the correlation level is equal to $L - m \times \phi$, where $m$ is the number of transmitters and $\phi$ is the absolute maximum cross-correlation, see \autoref{eq:pn-max-tx-pt1}.

		\begin{equation}
			\label{eq:pn-max-tx-pt1}
			L - m \times \phi > T
		\end{equation}

		\begin{equation}
			\label{eq:pn-max-tx-pt2}
			T > m \times \phi
		\end{equation}

		To prevent false negatives, i.e. there is no valid code present but due to for example MAI the correlation level suggests that there is a valid code present, see \autoref{eq:pn-max-tx-pt2} where $m$ is the number of transmitters and $\phi$ the absolute maximum cross-correlation.
		If we equalize \autoref{eq:pn-max-tx-pt1} and \autoref{eq:pn-max-tx-pt2} we can calculate what $m$ and $T$ are, those can be seen in \autoref{eq:m} and \autoref{eq:T}, respectively.

		\begin{equation}
			\label{eq:m}
			m = \frac{L}{2 \times \phi}
		\end{equation}

		\begin{equation}
			\label{eq:T}
			T = \frac{L}{2}
		\end{equation}

		The results are in \autoref{tbl:correlation-pn-families}.
		
		To guaranty that everything is decodable, without false positives and without false negatives, we can only have a small number of concurrent transmitters.
		In most cases we cannot even guaranty two simultaneous transmissions.
		It is for this reason, that the decoding scheme for the binary values of $0$ and $1$ is not further investigated in this section, but it is investigated in  \autoref{subsec:enc-dec-theory-gold}.







	\subsection{Gold Sequences}
	\label{subsec:enc-dec-theory-gold}


		The Gold sequences from the same family do have bounded cross-correlation, as can be seen \autoref{subsec:gold-theory}.
		So we can theorize what the maximum number of transmitters can be, without creating false-positives and/or false-negative.

		When we have \autoref{eq:m}, we know that in case of Gold codes, $L = 2^n - 1$ and that $\phi = t(n)$.
		This means: $m = \frac{2^n - 1}{2 \times t(n)}$, giving the results as shown in \autoref{tbl:correlation-gold-families}.


		\begin{table}[h]
			\centering
			\begin{tabular}{ | l | l | l | l | l | }

				\hline
				LFSR size 	& Code length	& Number of codes 	& Maximum cross-correlation & $m$	\\ \hline

				3			& 7				& 9					& 5							& 0.70	\\ \hline
				4			& 15			& 17				& 9							& 0.83	\\ \hline
				5			& 31			& 33				& 9							& 1.72	\\ \hline
				6			& 63			& 65				& 17						& 1.85	\\ \hline
				7			& 127			& 129				& 17						& 3.74	\\ \hline
				%8			& 255			& 257				& 33						& 3.86	\\ \hline is not a preferred pair for because mod 4 etc...
				9			& 511			& 513				& 33						& 7.74	\\ \hline
				10			& 1023			& 1025				& 65						& 7.87	\\ \hline	
				

			\end{tabular}
			\caption{Table containing the absolute maximum cross-correlation per Gold code family.}
			\label{tbl:correlation-gold-families}
		\end{table}

		As we can see from \autoref{tbl:correlation-gold-families}, the maximum cross-correlation for Gold codes is much better than that of the same-length PN codes.
		And as a direct result the maximum number of concurrent transmitters is also much higher for the same-length code, compared to the results from \autoref{tbl:correlation-pn-families}.

		Previously we said that if we wanted to encode data with these codes we would transmit the code for a $0$ databit and flip all the bits of the code for a $1$ databit, effectively XORing the code chips with the data.
		This works for the Walsh-Hadamard codes because they are synchronized. 
		And there is no cross-correlation between one transmitter which transmits a 0 and then a 1, with a second transmitter transmitting their data in overlap with both of the first transmitters codes.
		This can be the case when we use a-synchronous transmitters and for that case the cross-correlation function is not defined.
		Because when it is comparing two sequences and one sequence flips all its bits for a different databit, the correlation also changes. 
		For this reason, we will stick to just send the code itself and thereby only letting a receiver know that we are on or are off when we do not send a code.

		Next is the description on what happens when trying to decode a incoming signal which is a Gold code consisting of only $0$s and $1$s.

		\begin{proof}
			Let $s(t)$ be the received signal which is $c^b_i(t)$, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ and an arbitrary Gold code.\\
			And let $c^r_i(t)$ be the code for which we want to check if there is information there, where $c^r_i(t)$ uses the traditional values $-1$ and $+1$ and is the same Gold code as $c^b_i(t)$, only different values. \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^r_i(t)	
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^r_i(t)	
				\\ s(t) = c^b_i(t) 									
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c^b_i(t) \times c^r_i(t)
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \frac{1 - c^r_i(t)}{2} \times c^r_i(t) \tag{Via \autoref{eq:radio-to-bin}}
				\\ = \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) - c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) - \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) - \frac{1}{2} \times L
			\end{align*}

		\end{proof}

		We are unable to work out the the final remaining sum, because not all Gold sequences have the same sum.

		Next is the decoding result when there are $m$ concurrent transmitters, all transmitting with a separate Gold code.


		\begin{proof}
			Let $s(t)$ be the received signal which is are $m$ gold codes ($k = 1$ through $m$: $c^b_k(t)$, where $c^b_k(t)$ is binary valued, i.e. $0$ or $1$ and an arbitrary Gold code.\\
			And let $c^r_i(t)$ be the code for which we want to check if there is information there, where $c^r_i(t)$ uses the traditional values $-1$ and $+1$. \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^r_i(t)	
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^r_i(t)	
				\\ s(t) = \displaystyle\sum_{k = 1} ^ {m} c^b_k(t)						
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} \bigg\{ \displaystyle\sum_{k = 1} ^ {m} c^b_k(t) \bigg\} \times c^r_i(t)
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \displaystyle\sum_{k = 1} ^ {m} \bigg\{ \frac{1 - c^r_k(t)}{2} \times c^r_i(t) \bigg\} \tag{Via \autoref{eq:radio-to-bin}}
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \displaystyle\sum_{k = 1} ^ {m} \bigg\{ \frac{1}{2} \times c^r_i(t) - \frac{1}{2} \times c^r_k(t) \times c^r_i(t) \bigg\} 
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \bigg\{ \frac{m}{2} \times c^r_i(t) - \frac{1}{2} \times \displaystyle\sum_{k = 1} ^ {m} c^r_k(t) \times c^r_i(t) \bigg\} 
				\\ = \frac{m}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) - \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} \displaystyle\sum_{k = 1} ^ {m} \bigg\{ c^r_k(t) \times c^r_i(t) \bigg\}
			\end{align*}

		\end{proof}

		The first term of this results is dependent on the number of transmitters and the code used to decode the received signal.
		And the second term is the calculation of the correlation for each of the available signals in the received signal and then summing those correlations.

		To eliminate the first term entirely with normal Gold codes is impossible since the code length is uneven and therefore the sum of the code will always be non zero, because $\forall t \ \forall i \  | c^r_i(t) | = 1$.
		But the original Gold codes can be modified in such a way that the sum will be zero. 
		The first step is to remove all the codes that are not balanced, i.e. the number of $-1$s and $+1$s do not differ by one.
		Then we have codes that sum to $-1$ and the number of codes that remain is approximately half \cite{holmes2007spread}.
		For the next step we add a chip to each code such that the sum of the code is equal to zero.
		We have now affectively created a set of Orthogonal Gold codes.
		These codes have the property that the cross-correlation is zero for when they are synchronized, just like the Walsh-Hadamard codes.
		The only problem with these codes is that the cross- and autocorrelation properties are much worse than that of the `normal' Gold codes, as researched by \cite{5422934}, the absolute maximum cross-correlation approximately doubles.
		And as a direct result of this, the number of transmitters which can send simultaneous without false-positives and/or false-negatives at the decoding process also goes down by approximately a factor of two.
		So we can get rid of the first term by modifying the Gold codes, but we compromise a key factor in our system: the maximum number of concurrent transmitters without decoding errors, goes down significantly.

		Another way to get round the `problem' of the first term in the equation, is to always know what the result will be.
		That means that we have to know $m$, the number of transmitters and the sum of the code used to decode the received signal with.
		The sum of the code can be calculated each time a decoding takes place, so we can always know that.
		The problem remains with $m$. 
		So there needs to be an algorithm in place in the controller of each VLC enabled LED that makes sure that in every point in time a certain number of LEDs transmit their code.
		If such an algorithm can be constructed, then we are rid of the problem of the first term and if that number of LEDs is the maximum for which we can guaranty error free decoding, see \autoref{tbl:correlation-gold-families} column $m$, then we have solved two problems.


		To simulate what will happen when there are more transmitters concurrently transmitting than the number that is stated in \autoref{tbl:correlation-gold-families} per gold code length, a Matlab simulation is used.
		When simulating $m$ concurrent transmitters, $m$ random transmitters are selected out of the total $2^n + 1$ possible transmitters.
		Then for those $m$ transmitters a random offset $\gamma_i$ is chosen between $1$ and $L - 1 = 2^n - 2$, separate for each transmitter $i$.
		And then the last $\gamma_i$ chips are taken for each transmitters $i$ and transmitted.
		Next for each transmitter the entire code is transmitted.
		And finally another subset of chips is transmitted so that the total chips sent per transmitter is $2 \times (2^n - 1)$.
		This simulation makes sure that for $m$ transmitters they are all sent a-synchronously and that every transmitted code overlaps entirely with one or more other codes.
		This is needed because that is the way the cross-correlation is defined.

		Then for all the $2^n +1$ codes the decoding process starts, for each subset of the received signal where the $2^n - 1$ length code can fit.
		So that are the number of elements in the signal minus length of the code plus 1.
		For each of those position and for each code, the correlation is calculated.
		And then it is determined according to the offset that we know and the correlation number if that the number is a false-positive, false-negative, true-positive or true-negative.

		The assumption here is that there is a perfect channel.
		
		When there is a system for which we can classify the results as false/true-positives and false/true-negatives, we can evaluate the system's performance by investigating the precision and recall.
		These concepts are used in pattern recognition and information retrieval to evaluate an algorithm.
		The definitions of precision and recall can be found in \autoref{eq:precision} and \autoref{eq:recall} respectively, where $tp$ stands for the number of true-positives, $fp$ stand for the number of false-positives and $fn$ stand for the number of false-negatives.

		\begin{equation}
			\label{eq:precision}
			\text{Precision } = \frac{tp}{tp + fp}
		\end{equation}

		\begin{equation}
			\label{eq:recall}
			\text{Recall } = \frac{tp}{tp + fn}
		\end{equation}

		This method provides two metrics to consider.
		Ideally we want only one metric to consider and draw conclusion based on that.
		We can take the weighted average of the two metrics.
		This is called the F-measure and is the harmonic mean of the precision and recall.
		The definition of the F-measure can be seen in \autoref{eq:F-measure}.

		\begin{equation}
			\label{eq:F-measure}
			F = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}
		\end{equation}

		\begin{figure}[h]
			\centering
			\includegraphics[width=\textwidth]{chapters/f-measure-n=7-full-overlap-radio-a=1-100runs.eps}
			\caption{F measure plotted against the number of concurrent transmitters with $n = 7$, 100 runs, fully overlapping codes with Threshold $= \frac{L}{2}$.}
			\label{fig:f-measure-fully-overlapping-vs-num-of-tx}
		\end{figure}

		In \autoref{fig:f-measure-fully-overlapping-vs-num-of-tx} we can see the F-measure plotted against the number of transmitters that are transmitting concurrently, this is done for the Gold-code family with $n = 7$.
		We can see that the F-measure is equal to 1 up until the number of transmitters is equal to 4.
		This number can be derived via \autoref{eq:m}, and the solution is indeed 4 with the threshold being \autoref{eq:T}.
		When the number of transmitters increases, even as little as one additional transmitter, the F-measure gets very low. 
		At 5 transmitters, the F-measure equals to 0.2503. 
		This means that at any point in time there cannot be more than 4 concurrent transmitters, because else the number of errors when decoding will be very high as can be seen from the drop of the F-measure.

		Since there is no way to communicate to any of the VLC enabled LEDs to tell them when to start and stop transmitting, they have to decide that on their own. 
		The simplest way to make this decision is to have a probability $p$ for which it will transmit and with probability $1 - p$ it will not transmit, which is essentially a Bernoulli distribution.
		But since every transmitter is a Bernoulli distribution and we have more than one transmitter, the number of transmitters which will be transmitting at an point in time is a Binomial distribution.

		By giving the decision to each individual transmitter whether it is going to transmit or not gives us a problem.
		When the transmitters can transmit at every point in time, it is likely that the codes will not fully overlap.
		This means that we cannot use the correlation formulas, since they are defined for fully overlapping sequences.
		Because the sequences do not fully overlap we have to look at the aperiodic correlation, also known as the partial correlation as seen in \autoref{eq:aperiodic-correlation}.

		\begin{equation}
			R(\tau)_{xy} = 
				\begin{cases}
					\displaystyle\sum_{i = 0} ^ {L - 1 - \tau} x(i) \times y(i - \tau)		& \quad 0 \le \tau \le L - 1 \\
					\displaystyle\sum_{i = 0} ^ {L - 1 + \tau} x(i - \tau) \times y(i)		& \quad 1 - L \le \tau < L - 1 \\
					0																		& \quad  |\tau| \ge L
				\end{cases}
			\label{eq:aperiodic-correlation}
		\end{equation}

		Unfortunately the aperiodic correlation for a set of Gold codes is different than the periodic correlation for the same set.
		The general way to assert the aperiodic correlation for a set of Gold codes is just to calculate them.

		The simulation as described before is run again, but now with a slight modification.
		The offset stays but are now zeros instead of a subset of the code sequence.
		So the total length of the signal stays $2 \times (2^n - 1)$, but the codes do not overlap. 
		And the F-measure graph for this simulation can be seen in \autoref{fig:f-measure-no-overlapping-vs-num-of-tx}.

		\begin{figure}[h]
			\centering
			\includegraphics[width=\textwidth]{chapters/f-measure-n=7-no-overlap-radio-a=1-100runs.eps}
			\caption{F measure plotted against the number of concurrent transmitters with $n = 7$, 100 runs, no overlapping codes with Threshold $= \frac{L}{2}$.}
			\label{fig:f-measure-no-overlapping-vs-num-of-tx}
		\end{figure}

		We see the graph already dropping down before the $m = 4$ mark of the fully overlapping simulation from \autoref{fig:f-measure-fully-overlapping-vs-num-of-tx}.
		This is due to the increased correlation, namely the aperiodic correlation.
		The normal correlation of a $n = 7$ Gold code family is equal to $17$ (See \autoref{eq:gold-t(n)}), while the aperiodic correlation is equal to $28$.
		If we take \autoref{eq:m} and use $28$ for $\phi$, we end up with $2$ as the maximum number of concurrent transmitters such that the F-measure is still equal to 1, which is the same number as can be read from \autoref{fig:f-measure-no-overlapping-vs-num-of-tx}.


		Now that we have established that every transmitter will get a probability $p$ to transmit its code we will have to asses to following things: 

		\begin{itemize}
			\item What should $p$ be in order to guaranty, to a certain degree, that we will be able to successfully decode all data at every point in time ?
			\item Will every transmitter transmit their code, and if so within what time frame ?
			\item What is the total time for which we can guaranty, to a certain degree, such that every transmitter has transmitter their code at least once ? 
		\end{itemize}




		The time it takes for a group of $m$ transmitters to transmit their code can be seen in \autoref{eq:time-m-tx}, where $L$ is the length of the code and $f$ is the frequency at which the code will be sent. 

		\begin{equation}
			\label{eq:time-m-tx}
			t_{m} = \frac{L}{f}
		\end{equation}

		The lower-bound of the total time is the group time times the number of groups, since every group has the size of $m$ transmitters the number of groups will be $\frac{N}{m}$, where $N$ is the total number of transmitters, see \autoref{eq:total-time-tx}.
		But this is the lower bound and it does not guaranty, to a certain degree, that every transmitter will have transmitted their code at least once.

		\begin{equation}
			\label{eq:total-time-tx}
			t \ge t_{m} \times \frac{N}{m} = \frac{L}{f} \times \frac{N}{m}
		\end{equation}




		We can choose the probability $p$ in such a way that the mean of the Binomial distribution is equal to $m$, the maximum number of concurrent transmitters for which the F-measure is still 1.
		So that the total time it takes for every LED to transmit their code is as small as possible.
		The number of transmitters at each point in time will follow a Binomial distribution as discussed above. As an example we will use $n = 7$, meaning $N = 2^n + 1 = 129$, $L = 2^n - 1 = 127$.
		Since we want the mean of the Binomial distribution to be $m$ and the mean is equal to the number of trials times the probability, the probability will be: $p = \frac{m}{N}$.
		With all parameters known of the binomial distribution we can calculate the probability mass function, see \autoref{fig:pmf-n=7} and the cumulative distribution function, see \autoref{fig:cdf-n=7}.

		\begin{figure}[h!]
			\centering
			\includegraphics[width=\textwidth]{chapters/pmf-n=7-p=m_over_N.eps}
			\caption{Probability mass function with $n = 7$ and $p = \frac{m}{N}$ of the number of concurrent transmitters.}
			\label{fig:pmf-n=7}
		\end{figure}

		\begin{figure}[h!]
			\centering
			\includegraphics[width=\textwidth]{chapters/cdf-n=7-p=m_over_N.eps}
			\caption{Cumulative distribution function with $n = 7$ and $p = \frac{m}{N}$  of the number of concurrent transmitters.}
			\label{fig:cdf-n=7}
		\end{figure}

		As we can see from the probability mass function (\autoref{fig:pmf-n=7}), the probability for $m = 2$ number of transmitters is one of the highest, but there are also probabilities for $m > 2$.
		Which will cause problems as can be seen in \autoref{fig:f-measure-no-overlapping-vs-num-of-tx} when $m > 2$.
		If we look at the cumulative distribution function (\autoref{fig:cdf-n=7}) the probability of the number of concurrent transmitters equal or lower to $m = 2$ is roughly 62 \%, this means that the probability that the number of concurrent transmitters will be higher than $m = 2$ is roughly equal to 38 \%. 
		That means that 38 \% of the time we will have problems with decoding.
		This means that the probability will have to be changed in order to guaranty, to a certain degree, that at every point in time the number of concurrent transmitters will not exceed $m$.
		The cumulative distribution function for a binomial distribution can be seen in \autoref{eq:cdf-binomial}, where $X$ is the random variable for the number of transmitters at every point in time, $m$ is the maximum number of transmitter allowed for good decoding and $N$ is the total number of transmitters used.


		\begin{equation}
			\label{eq:cdf-binomial}
			\text{CDF:  PR}(X \le m) = \displaystyle\sum_{i=0}^{m} \binom Ni \times p^i \times (1 - p)^{N-i}
		\end{equation}

		We want the probability that $X \le m$ to be as high as possible, but since the CDF goes asymptotically to $1$, we cannot choose the probability $1$.
		So we choose a number that is as good as $1$, namely $1 - \epsilon$.
		We can then equalize $1 - \epsilon$ to \autoref{eq:cdf-binomial} and find a probability $p$, since every other variable is known.
		With the found $p$ we can say that $1 - \epsilon$ part of the time the number of concurrent transmitters will be less than or equal to $m$.


		Since every transmitter transmits their code with probability $p$ there is a probability that it will never transmit its code.
		So what we want to know is after how many tries can we guaranty, to a certain degree, that every transmitter will have transmitted their code at least once.
		This follows a geometric distribution.
		For the same probability $p = \frac{m}{N}$ the PMF, \autoref{fig:pmf-n=7-num-of-tries} and the CDF, \autoref{fig:cdf-n=7-num-of-tries} can be seen.

		\begin{figure}[!h]
			\centering
			\includegraphics[width=\textwidth]{chapters/pmf-n=7-p=m_over_N_num_of_tries.eps}
			\caption{Probability mass function with $n = 7$ and $p = \frac{m}{N}$ of the number of tries and the first success of transmission.}
			\label{fig:pmf-n=7-num-of-tries}
		\end{figure}

		\begin{figure}[!h]
			\centering
			\includegraphics[width=\textwidth]{chapters/cdf-n=7-p=m_over_N_num_of_tries.eps}
			\caption{Cumulative distribution function with $n = 7$ and $p = \frac{m}{N}$ of the number of tries and the first success of transmission.}
			\label{fig:cdf-n=7-num-of-tries}
		\end{figure}


		The CDF of a geometric distribution can be expressed as seen in \autoref{eq:cdf-geometric}, where $Y$ is the number of attempts until the first transmission, $p$ is the probability that the transmitter will transmit its code and $k$ is the number attempts until the first transmission for which you want to know the probability.
		\begin{equation}
			\label{eq:cdf-geometric}
			\text{CDF:  Pr}(Y \le k) = 1 - (1 - p)^k
		\end{equation}
		
		We want this probability, for a given $k$, to be as high as possible but as we see from the CDF (\autoref{fig:cdf-n=7-num-of-tries}), the probability that a transmitter transmits at least once goes asymptotically to $1$, so we cannot choose the probability $1$.
		So we choose again a number that is as good as $1$, namely $1 - \epsilon_1$.
		We can now express $k$ as can be seen in \autoref{eq:reverse-cdf-geometric}.

		\begin{equation}
			\label{eq:reverse-cdf-geometric}
			k = \frac{\ln\epsilon_1}{\ln(1 - p)}
		\end{equation}

		So after $k$ attempts the probability that a transmitter will have transmitted its code at least once is $1 - \epsilon_1$.
		This holds for one transmitter, but since each transmitter works in parallel and each random variable is independent and identically distributed, this distribution holds for the entire system with $N$ transmitters.

		\autoref{eq:total-time-tx} gives the lower bound of the total time needed for all the transmitters to transmit at least once.
		But this is a lower bound which is not realistic as this bound does not take into account that the transmitter will actually transmit its code.
		From the cumulative distribution function (\autoref{fig:cdf-n=7-num-of-tries}) at the y-axis we can see that if we want to have a probability of for example 90 \% for each transmitter to transmit at least once, we need approximately 150 tries. 
		Which is significantly more than the $\frac{N}{m} = \frac{129}{2} = 65$ times we saw above.

		Now we can give a better estimation of the total time it will take, see \autoref{eq:better-total-time-tx}.

		\begin{equation}
			\label{eq:better-total-time-tx}
			t \ge \frac{L}{f} \times k = \frac{L}{f} \times \frac{\ln(\epsilon_1)}{\ln(1 - p)}
		\end{equation}



		

		

		

		
		







