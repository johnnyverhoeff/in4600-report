%!TEX root = ../thesis.tex

\section{Encoding \& Decoding Data Simulation}
\label{sec:enc-dec-theory}

	All of the coding schemes as discussed in \autoref{sec:CDMA} are designed for wireless transceivers.
	Meaning they send a radio wave through the air which is encoded with that particular code sequence.
	That radio wave is an analog signal and can vary between $+1$ and $-1$ symbols.
	But the VLC enabled LEDs cannot send such signals.
	They can only be turned on or off, thus encoding only $0$ and $1$ symbols.
	Those symbols correspond to a current draw of the LED which is measured by an ADC for example.
	To experiment with the encoding and decoding process and obstacles without the need for hardware, we use Matlab to simulate the process.

	\subsection{Walsh-Hadamard Sequences}

		%about radio dec. and bin. enc. proof etc...
		The way the Walsh-Hadamard sequences work with encoding and decoding with for example the link from a base-station to a mobile device of a CDMA application, is that the sequence of $+1$ and $-1$ symbols gets multiplied with the encoded data. 
		Say for example that we have a Hadamard matrix of rank $8$ as can be seen in \autoref{matrix:h8} which is created by using \autoref{eq:hadamard-matrix-creation}.
		And we select a random code to encode our data with, which corresponds to a row of this matrix.
		We also assume that our data is binary, consisting of 0s and 1s. 
		Our last assumption is that is we want to encode a $0$ we use the code itself and if we want to encode a $1$ we use the inverse of the code, i.e. $-1 \times$ the code.
		That describes the encoding process. 
		The decoding process involves the calculation of the correlation of the received signal with a code sequence for which we want to know if there is information embedded in the received signal and if so, what might that information be.

			\begin{equation}
				H_8 = 
				\begin{bmatrix*}[r] 
					H_4 & H_4 \\ 
					H_4 & -H_4 
				\end{bmatrix*}
				%=
				%\begin{bmatrix*}[r] 
				%	H_2 & H_2   & H_2 & H_2   \\
				%	H_2 & -H_2  & H_2 & -H_2  \\
				%	H_2 & H_2   & -H_2 & -H_2 \\
				%	H_2 & -H_2  & -H_2 & H_2  \\
				%\end{bmatrix*}
				=
				\dotsc
				=
				\begin{bmatrix*}[r]
					1	&	 1	&	 1	&	 1	&	 1	&	 1	&	 1	&	 1 \\
					1	&	-1	&	 1	&	-1	&	 1	&	-1	&	 1	&	-1 \\
					1	&	 1	&	-1	&	-1	&	 1	&	 1	&	-1	&	-1 \\
					1	&	-1	&	-1	&	 1	&	 1	&	-1	&	-1	&	 1 \\
					1	&	 1	&	 1	&	 1	&	-1	&	-1	&	-1	&	-1 \\
					1	&	-1	&	 1	&	-1	&	-1	&	 1	&	-1	&	 1 \\
					1	&	 1	&	-1	&	-1	&	-1	&	-1	&	 1	&	 1 \\
					1	&	-1	&	-1	&	 1	&	-1	&	 1	&	 1	&	-1 
				\end{bmatrix*}
				\label{matrix:h8}
			\end{equation}

		Since the autocorrelation of Walsh-Hadamard sequences do not allow to find the beginning of a code, see \autoref{fig:autocorr-hadamard}, we will only consider the synchronized situation, for which these codes are also used in practice, so $\tau = 0$.
		Say we have a code $c_i(t)$ and we want to encode a $0$ with this code and send that as a signal for our receiver to receive.
		The receiver will receive this signal $s(t)$ assuming a perfect channel, the decoding process can be seen below.

		%Want something different than proof, but dont know how/what

		\begin{proof}
			Let $s(t)$ be the received signal which encodes a 0 for code: $c_i(t)$.\\
			And let $c_i(t)$ be the code for which we want to check if there is information there. \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c_i(t)	
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c_i(t)	
				\\ s(t) = c_i(t)															
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c_i(t) \times c_i(t) = L
			\end{align*}

		\end{proof}

		We can also state what the result will be when we decode a $1$ with a code $c_i(t)$, see below:

		\begin{proof}
			Let $s(t)$ be the received signal which encodes a 1 for code: $c_i(t)$.\\
			And let $c_i(t)$ be the code for which we want to check if there is information there. \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c_i(t)	
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c_i(t)	
				\\ s(t) = -1 \times c_i(t)															
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} -1 \times c_i(t) \times c_i(t)
				\\ &= -1 \times \displaystyle\sum_{t = 0} ^ {L - 1} c_i(t) \times c_i(t) = -1 \times L
			\end{align*}

		\end{proof}

		This two statement state that if we encode a data bit with code $c_i$ and that is the only information received by the receiver, we either get the maximum positive correlation, i.e. $L$ or we get the maximum negative correlation, i.e. $-L$.
		If we would have received a signal consisting of only zeros the correlation would also be zero.

		Next let us look at what happens when we try to decode information with a different code than the code which was used to encode information with. In that case we also get a correlation of zero, see below. This is per definition the case since we are using Walsh-Hadamard codes which are orthogonal to each other when they are synchronized.

		\begin{proof}
			Let $s(t)$ be the received signal which encodes a data bit for code: $c_i(t)$.\\
			And let $c_j(t)$ be the code for which we want to check if there is information there. \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c_j(t)	
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c_j(t)	
				\\ s(t) = c_i(t)															
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c_i(t) \times c_j(t) = 0
			\end{align*}

		\end{proof}

		Finally we look at the decoding process for when the received signal consists of several encoded signals, see below. 
		Because all the codes in Walsh-Hadamard sequences are orthogonal to each other they produce no MAI and so there can be as many codes superimposed on each other and still a correct decoding can take place. 


		\begin{proof}
			Let $s(t)$ be the received signal which encodes data for several codes: $c_i(t)$, $c_j(t)$, $c_k(t)$ and $c_l(t)$.\\
			And let $c_i(t)$ be the code for which we want to check if there is information there. \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c_i(t)	
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c_i(t)	
				\\ s(t) = c_i(t) + c_j(t) + c_k(t) + c_l(t)															
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} \{ c_i(t) + c_j(t) + c_k(t) + c_l(t) \} \times c_i(t)
				\\ R(0)_{sc_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c_i(t) \times c_i(t) + c_j(t) \times c_i(t) + c_k(t) \times c_i(t) + c_l(t) \times c_i(t)
				\\ = L + 0 + 0 + 0 = L
			\end{align*}

		\end{proof}

		But as stated in \autoref{sec:enc-dec-theory} the VLC enabled LEDs cannot send $+1$ and $-1$ symbols of data, they can only be in an on or off state.
		This means that the code that is being used for encoding must be mapped to zeros and ones.
		Let us assume that a $+1$ symbol maps to an off state, i.e. $0$ and that a $-1$ symbol maps to an on state, i.e. $1$.
		We can summarize that in \autoref{eq:radio-to-bin}, where $r$ denotes the $+1$ or $-1$ symbols and the outcome $b$ will be our binary value, i.e. $0$ or $1$.

		\begin{equation}
			b = \frac{1 - r}{2}
			\label{eq:radio-to-bin}
		\end{equation}

		Because we are now using the binary values as code, we can no longer use multiply to encode our binary information.
		Instead, we must use th XOR function.
		Now let us see what happens when we try to decode information that has been encoded using on-off keying (OOK), below is stated what happens when both data bits are encoded.

		\begin{proof}
			Let $s(t)$ be the received signal which encodes a 0 for code: $c^b_i(t)$, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\
			And let $c^r_i(t)$ be the code for which we want to check if there is information there, where $c^r_i(t)$ uses the traditional values $-1$ and $+1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^r_i(t)	
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^r_i(t)	
				\\ s(t) = \forall t \in \{ 0, 1, 2, \dotsc, L - 1 \} \ \ 0(t) \oplus c^b_i(t) = c^b_i(t) \tag{Where $0(t) = 0 \ \forall t$ }										
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c^b_i(t) \times c^r_i(t)
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \frac{1 - c^r_i(t)}{2} \times c^r_i(t) \tag{Via \autoref{eq:radio-to-bin}}
				\\ = \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) - c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) - \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t)
				\\ = 0 - \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t) \tag{Balance property}
				\\ = - \frac{1}{2} \times L
			\end{align*}

		\end{proof}

		\begin{proof}
			Let $s(t)$ be the received signal which encodes a 1 for code: $c^b_i(t)$, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\
			And let $c^r_i(t)$ be the code for which we want to check if there is information there, where $c^r_i(t)$ uses the traditional values $-1$ and $+1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^r_i(t)	
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^r_i(t)	
				\\ s(t) = \forall t \in \{ 0, 1, 2, \dotsc, L - 1 \} \ \ 1(t) \oplus c^b_i(t) = 1 - c^b_i(t) \tag{Where $1(t) = 1 \ \forall t$ }
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} (1 - c^b_i(t)) \times c^r_i(t)
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} (1 - \frac{1 - c^r_i(t)}{2}) \times c^r_i(t) \tag{Via \autoref{eq:radio-to-bin}}
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \frac{1 + c^r_i(t)}{2} \times c^r_i(t)
				\\ = \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) + c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) + \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t)
				\\ = 0 + \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t) \tag{Balance property}
				\\ = \frac{1}{2} \times L
			\end{align*}

		\end{proof}

		The case for when there is no data encoded for this particular code:

		\begin{proof}
			Let $s(t)$ be the received signal which encodes data for code: $c^b_i(t)$, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\
			And let $c^r_j(t)$ be the code for which we want to check if there is information there, where $c^r_j(t)$ uses the traditional values $-1$ and $+1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^r_j(t)	
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^r_j(t)	
				\\ s(t) = c^b_i(t)										
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c^b_i(t) \times c^r_j(t)
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \frac{1 - c^r_i(t)}{2} \times c^r_j(t) \tag{Via \autoref{eq:radio-to-bin}}
				\\ = \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_j(t) - c^r_i(t) \times c^r_j(t)
				\\ = \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_j(t) - \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_j(t)
				\\ = 0 - \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t) \tag{Balance property}
				\\ = 0 + 0 = 0\tag{Because the codes are orthogonal.}
			\end{align*}

		\end{proof}

		Finally the case for when there are multiple signals:


		\begin{proof}
			Let $s(t)$ be the received signal which encodes data for several codes: $c^b_i(t)$, $c^b_j(t)$, $c^b_k(t)$ and $c^b_l(t)$, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$\\
			And let $c^r_i(t)$ be the code for which we want to check if there is information there, where $c^r_i(t)$ uses the traditional values $-1$ and $+1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^r_i(t)	
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^r_i(t)	
				\\ s(t) = c^b_i(t) + c^b_j(t) + c^b_k(t) + c^b_l(t)															
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} \{ c^b_i(t) + c^b_j(t) + c^b_k(t) + c^b_l(t) \} \times c^r_i(t)
				\\ R(0)_{sc^r_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c^b_i(t) \times c^r_i(t) + c^b_j(t) \times c^r_i(t) + c^b_k(t) \times c^r_i(t) + c^b_l(t) \times c^r_i(t)
				\\ = \frac{1}{2} \times L + 0 + 0 + 0 = \frac{1}{2} \times L \tag{See above proofs}
			\end{align*}

		\end{proof}




		As we can see the correlation is for both data bits and when there is no information encoded, exactly $-\frac{1}{2}$ times as large as when dealing only with $+1$ and $-1$ symboled codes. 
		It also works for when there are multiple streams of encoded information present in the signal received.
		A summary is available, see \autoref{tbl:correlation-levels}.

		\begin{table}[h!]
			\centering
			
			\begin{tabular}{| l | l |}
				\hline
				Method												& Correlation \\ \hline \hline
				Encoding a 0 and decoding with only one transmitter & $-\frac{1}{2} \times L$  \\ \hline
				Encoding a 1 and decoding with only one transmitter & $ \frac{1}{2} \times L$   \\ \hline
				Encoding a 0 and decoding with a different code, with only one transmitter & 0  \\ \hline
				Encoding a 0 and decoding with multiple transmitters & $-\frac{1}{2} \times L$ \\ \hline
			\end{tabular}
			\caption{Table containing summary of correlation levels.}
			\label{tbl:correlation-levels}
		\end{table}









		We can also choose to encode and decode with the binary valued codes only, so only use values with $0$ and $1$. 


		\begin{proof}
			Let $s(t)$ be the received signal which encodes a 0 for code: $c^b_i(t)$, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\
			And let $c^b_i(t)$ be the code for which we want to check if there is information there, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^b_i(t)	
				\\ R(0)_{sc^b_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^b_i(t)	
				\\ s(t) = \forall t \in \{ 0, 1, 2, \dotsc, L - 1 \} \ \ 0(t) \oplus c^b_i(t) = c^b_i(t) \tag{Where $0(t) = 0 \ \forall t$ }
				\\ R(0)_{sc^b_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c^b_i(t) \times c^b_i(t)
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \frac{1 - c^r_i(t)}{2} \times \frac{1 - c^r_i(t)}{2} \tag{Via \autoref{eq:radio-to-bin}}
				\\ = \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} 1 - 2 \times c^r_i(t) + c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} 1 - \frac{1}{2} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) + \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{4} \times L - 0 + \frac{1}{4} \times L = \frac{1}{2} \times L
			\end{align*}

		\end{proof}


		\begin{proof}
			Let $s(t)$ be the received signal which encodes a 1 for code: $c^b_i(t)$, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\
			And let $c^b_i(t)$ be the code for which we want to check if there is information there, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^b_i(t)	
				\\ R(0)_{sc^b_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^b_i(t)	
				\\ s(t) = \forall t \in \{ 0, 1, 2, \dotsc, L - 1 \} \ \ 1(t) \oplus c^b_i(t) = 1 - c^b_i(t) \tag{Where $1(t) = 1 \ \forall t$ }
				\\ R(0)_{sc^b_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} (1 - c^b_i(t)) \times c^b_i(t)
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} (1 - \frac{1 - c^r_i(t)}{2}) \times \frac{1 - c^r_i(t)}{2} \tag{Via \autoref{eq:radio-to-bin}}
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \frac{1 + c^r_i(t)}{2} \times \frac{1 - c^r_i(t)}{2}
				\\ = \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} 1 - c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} 1 - \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{4} \times L - \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_i(t)
				\\ = \frac{1}{4} \times L - \frac{1}{4} \times L = 0
			\end{align*}

		\end{proof}

		\begin{proof}
			Let $s(t)$ be the received signal which encodes a 0 for code: $c^b_i(t)$, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\
			And let $c^b_j(t)$ be the code for which we want to check if there is information there, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^b_j(t)	
				\\ R(0)_{sc^b_{j}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^b_j(t)	
				\\ s(t) = \forall t \in \{ 0, 1, 2, \dotsc, L - 1 \} \ \ 0(t) \oplus c^b_i(t) = c^b_i(t) \tag{Where $0(t) = 0 \ \forall t$ }
				\\ R(0)_{sc^b_{j}} = \displaystyle\sum_{t = 0} ^ {L - 1} c^b_i(t) \times c^b_j(t)
				\\ = \displaystyle\sum_{t = 0} ^ {L - 1} \frac{1 - c^r_i(t)}{2} \times \frac{1 - c^r_j(t)}{2} \tag{Via \autoref{eq:radio-to-bin}}
				\\ = \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} 1 - c^r_i(t) - c^r_j(t) + c^r_i(t) \times c^r_j(t)
				\\ = \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} 1 - \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) - \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_j(t) + \frac{1}{4} \times \displaystyle\sum_{t = 0} ^ {L - 1} c^r_i(t) \times c^r_j(t) 
				\\ = \frac{1}{4} \times L - 0 - 0 + 0 = \frac{1}{4} \times L
			\end{align*}

		\end{proof}

		\begin{proof}
			Let $s(t)$ be the received signal which encodes data for codes: $c^b_j(t)$, $c^b_l(t)$, where $c^b_j(t)$ is binary valued, i.e. $0$ or $1$\\
			And let $c^b_i(t)$ be the code for which we want to check if there is information there, where $c^b_i(t)$ is binary valued, i.e. $0$ or $1$ \\

			\begin{align*}
				R(\tau)_{xy} = \displaystyle\sum_{t = 0} ^ {L - 1} x(t) \times y(t + \tau)	\tag{See \autoref{eq:correlation}}
				\\ \tau = 0,\ x = s(t),\ y = c^b_i(t)	
				\\ R(0)_{sc^b_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} s(t) \times c^r_i(t)	
				\\ s(t) = c^b_j(t) + c^b_l(t)														
				\\ R(0)_{sc^b_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} \{ c^b_j(t) + c^b_l(t) \} \times c^b_i(t)
				\\ R(0)_{sc^b_{i}} = \displaystyle\sum_{t = 0} ^ {L - 1} c^b_j(t) \times c^b_i(t) + c^b_l(t) \times c^b_i(t)
				\\ = \frac{1}{4} \times L + \frac{1}{4} \times L = \frac{1}{2} \times L \tag{See above proofs}
			\end{align*}

		\end{proof}



		\begin{table}[h!]
			\centering
			
			\begin{tabular}{| l | l |}
				\hline
				Method												& Correlation \\ \hline \hline
				Encoding a 0 and decoding with only one transmitter 							& $\frac{1}{2} \times L$  \\ \hline
				Encoding a 1 and decoding with only one transmitter 							& $ 0 $   \\ \hline
				Encoding a 0 and decoding with a different code, with only one transmitter 		& $\frac{1}{4} \times L$  \\ \hline
				Encoding a 0 and decoding with a different code, with two transmitters			& $\frac{1}{2} \times L$  \\ \hline
			\end{tabular}
			\caption{Table containing summary of correlation levels.}
			\label{tbl:correlation-levels-bin}
		\end{table}

		As we can see from the summary in \autoref{tbl:correlation-levels-bin}, the correlation levels for decoding one transmitter with the correct code, is the same as the correlation level from decoding with a code that was not encoded.
		This forms a problem as we cannot distinguish correctly these cases.
		And as a result we cannot use this decoding scheme. 


		In the synchronous case, for which these codes are also used in practice, we can correctly encode and subsequently decode information. 
		Without experiencing false-positives and without false-negatives.
		There can also be as much transmitters transmitting concurrently without MAI as there are codes available, because the codes are orthogonal.
		The only downside to this scheme is that all of the transmitter have to work together, they have to transmit synchronously.





	\subsection{PN Sequences}

		As we know from \autoref{sec:theory-pn}, there is no particular bound on the cross-correlation between two or more PN sequences.
		This means that they will have MAI and that there is a limit on how many transmitters can transmit at the same time, with the the receiver still able to correctly decode the information without experiencing false-positives and/or false-negatives.

		Since there is no particular bound on the cross-correlation with PN sequences from the same length, they need to be calculated.
		The calculated results can be seen in \autoref{tbl:correlation-pn-families}.


		\begin{table}
			\centering
			\begin{tabular}{ | l | l | l | l | l | }

				\hline
				LFSR size 	& Code length	& Number of codes 	& Maximum cross-correlation & $m$	\\ \hline

				3			& 7				& 2					& 5							& 0.70	\\ \hline	
				4			& 15			& 2					& 7							& 1.07	\\ \hline
				5			& 31			& 6					& 11						& 1.41	\\ \hline
				6			& 63			& 6					& 23						& 1.37	\\ \hline
				7			& 127			& 18				& 43						& 1.48	\\ \hline
				8			& 255			& 16				& 95						& 1.34	\\ \hline	


			\end{tabular}
			\caption{Table containing the absolute maximum cross-correlation per PN code family.}
			\label{tbl:correlation-pn-families}
		\end{table}

		If we want to calculate how many simultaneous transmitters can transmit, we need to set a threshold $T$ for which we will accept or reject a correlation as being a valid transmitter or not.
		We also know the correlation level of when a code is present in the signal, this is equal to $L$, where $L$ is the length of the code.
		The last thing we need is the absolute maximum cross-correlation for a given PN family of sequences and we can calculate the maximum number of concurrent transmitters.
		To prevent false negatives, i.e. there is a valid code present but it is lost due to for example MAI, the correlation level needs to be higher than the threshold $T$. 
		And the correlation level is equal to $L - m \times \phi$, where $m$ is the number of transmitters and $\phi$ is the absolute maximum cross-correlation, see \autoref{eq:pn-max-tx-pt1}.

		\begin{equation}
			\label{eq:pn-max-tx-pt1}
			L - m \times \phi > T
		\end{equation}

		\begin{equation}
			\label{eq:pn-max-tx-pt2}
			T > m \times \phi
		\end{equation}

		To prevent false negatives, i.e. there is no valid code present but due to for example MAI the correlation level suggests that there is a valid code present, see \autoref{eq:pn-max-tx-pt2} where $m$ is the number of transmitters and $\phi$ the absolute maximum cross-correlation.
		If we equalize \autoref{eq:pn-max-tx-pt1} and \autoref{eq:pn-max-tx-pt2} we can calculate what $m$ and $T$ are, those can be seen in \autoref{eq:m} and \autoref{eq:T}, respectively.

		\begin{equation}
			\label{eq:m}
			m = \frac{L}{2 \times \phi}
		\end{equation}

		\begin{equation}
			\label{eq:T}
			T = \frac{L}{2}
		\end{equation}

		The results are in \autoref{tbl:correlation-pn-families}.
		
		To guaranty that everything is decodable, without false positives and without false negatives, we can only have a small number of concurrent transmitters.
		In most cases we cannot even guaranty two simultaneous transmissions.







	\subsection{Gold Sequences}

		The Gold sequences from the same family do have bounded cross-correlation, as can be seen \autoref{subsec:gold-theory}.
		So we can theorize what the maximum number of transmitters can be, without creating false-positives and/or false-negative.

		When we have \autoref{eq:m}, we know that in case of Gold codes, $L = 2^n - 1$ and that $\phi = t(n)$.
		This means: $m = \frac{2^n - 1}{2 \times t(n)}$, giving the results as shown in \autoref{tbl:correlation-gold-families}.


		\begin{table}[h]
			\centering
			\begin{tabular}{ | l | l | l | l | l | }

				\hline
				LFSR size 	& Code length	& Number of codes 	& Maximum cross-correlation & $m$	\\ \hline

				3			& 7				& 9					& 5							& 0.70	\\ \hline
				4			& 15			& 17				& 9							& 0.83	\\ \hline
				5			& 31			& 33				& 9							& 1.72	\\ \hline
				6			& 63			& 65				& 17						& 1.85	\\ \hline
				7			& 127			& 129				& 17						& 3.74	\\ \hline
				8			& 255			& 257				& 33						& 3.86	\\ \hline
				9			& 511			& 513				& 33						& 7.74	\\ \hline
				10			& 1023			& 1025				& 65						& 7.87	\\ \hline	
				

			\end{tabular}
			\caption{Table containing the absolute maximum cross-correlation per Gold code family.}
			\label{tbl:correlation-gold-families}
		\end{table}

		As we can see from \autoref{tbl:correlation-gold-families}, the maximum cross-correlation for Gold codes is much better than that of the same-length PN codes.
		Ans as a direct result the maximum number of concurrent transmitters is also much higher for the same-length code.


