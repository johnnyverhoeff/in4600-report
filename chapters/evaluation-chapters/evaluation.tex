% !TeX root = ../../thesis.tex

\chapter{Evaluation}
\label{chp:evaluation}




Now that the IDs that will be assigned to each LED have been investigated in \autoref{chp:cdma} and the hardware testbeds to encode the IDs and to measure the aggregated energy consumption have been explained in \autoref{chp:hardware-design}, it is time to evaluate the entire system.
In this chapter, we evaluate the accuracy and timing of the system to detect if the lights are on and by looking at the aggregated energy consumption.
Two small scale systems will be evaluated.
The first system is with the DC testbed which has six lights.
And the second system is the AC testbed which has three lights.
These are only small scale systems, so to test the setup with a greater number of lights, software simulations are used to evaluate the accuracy and timing of a system with more than 100 lights.




In every setup, each light will be assigned an ID.
This ID is shared with the smart-meter.%, so that it knows which light corresponds with which ID.
The energy data that the smart-meter samples, is correlated with each ID and the outcome of that calculation will determine if the corresponding light is on or off.
As was explained in \autoref{sec:interference-solution} the outcome of the correlation calculation is compared to a threshold.
The outcome can either be less than or equal to the threshold or it can be greater than the threshold.
For each of these cases, the outcome of the calculation can be correct or incorrect.
For example, there can be too much interference so that one of the lights gets mistakenly identified as being on, while in reality it is off.
So in total four categories must be considered, where the correlation outcome is denoted by $R$ and the threshold is denoted by $T$:




\begin{itemize}

	\item True Positive: $R > T$, indicating the light is on and the light is actually on.

	\item False Positive: $R > T$, indicating the light is on but the light is actually off.

	\item True Negative: $R \le T$, indicating the light is off and the light is actually off.

	\item False Negative: $R \le T$, indicating the light is off but the light is actually on.


\end{itemize}




When there is a system for which we can classify the results as false/true-positives and false/true-negatives, we can evaluate the system's performance by investigating the precision and recall.
The definitions of precision and recall can be found in \autoref{eq:precision} and \autoref{eq:recall} respectively, where $tp$ stands for the number of true-positives, $fp$ stand for the number of false-positives and $fn$ stand for the number of false-negatives.

\begin{equation}
	\label{eq:precision}
	\text{Precision } = \frac{tp}{tp + fp}
\end{equation}

\begin{equation}
	\label{eq:recall}
	\text{Recall } = \frac{tp}{tp + fn}
\end{equation}

This method provides two metrics to consider.
Ideally we would like only one metric and draw conclusions based on that.
We can take the weighted average of the two metrics.
This is called the F-measure \cite{sokolova2009systematic} and is the harmonic mean of the precision and recall.
The definition of the F-measure can be seen in \autoref{eq:F-measure}.
When the F-measure is equal to 1, the system has perfect accuracy.
The lower the F-measure is, the more false-negatives and/or false-positives have occurred.

\begin{equation}
	\label{eq:F-measure}
	F = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}
\end{equation}



The next sections will explain the evaluation of the hardware testbeds and the software simulation.


\input{chapters/evaluation-chapters/hardware/hardware-evaluation}


\input{chapters/evaluation-chapters/simulation/simulation-evaluation}



